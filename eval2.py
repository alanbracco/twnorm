"""Evaluate normalization.

Usage:
  eval.py -r <gold_file> -g <generated_file> [-o <output_file>]
  eval.py -h | --help

Options:
  -r <gold_file>      Original corpus to compare results
  -g <generated_file> File generated by the normalizator
  -o <output_file>     Output file with normalization performance info
                       [default: stats.txt]
  -h --help            Show this screen.
"""
import os
import sys
from copy import copy
from docopt import docopt
from tweets_splitter import Tw_Splitter
from wta_picker import WTApickereval


if __name__ == '__main__':

    opts = docopt(__doc__)

    gold_file = opts['-r']
    gold_file_path = os.path.join(os.getcwd(), 'Input', gold_file)
    if not os.path.isfile(gold_file_path):
        print('You must enter an existing input file.')
        sys.exit()

    generated_file = opts['-g']
    generated_file_path = os.path.join(os.getcwd(), 'Output', generated_file)
    if not os.path.isfile(generated_file_path):
        print('You must enter an existing input file.')
        sys.exit()

    output_file = opts['-o']
    if output_file is None:
        output_file = 'stats.txt'
    output_file = 'Stats/' + output_file
    if os.path.exists(output_file):
        os.remove(output_file)

    gold_splitter = Tw_Splitter(gold_file_path)
    gold_dict = gold_splitter.corrections

    generated_splitter = Tw_Splitter(generated_file_path)
    generated_dict = generated_splitter.corrections

    all_tokens = WTApicker(gold_splitter.texts).all_tokens

    evaluator = Evaluator(output_file)
    evaluator.get_measure(gold_dict, generated_dict, all_tokens)
